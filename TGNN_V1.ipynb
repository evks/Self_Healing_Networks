{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "U4U-Hh1Tg4Kr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import math\n",
        "\n",
        "from torch.nn import LayerNorm, BatchNorm1d"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def drop_edge(edge_index, edge_attr, drop_prob=0.2):\n",
        "    if not self.training or drop_prob == 0.0:\n",
        "        return edge_index, edge_attr\n",
        "\n",
        "    mask = torch.rand(edge_index.size(1)) > drop_prob\n",
        "    return edge_index[:, mask], edge_attr[mask]"
      ],
      "metadata": {
        "id": "Q5Qn8UB1g_EE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EGATLayer(nn.Module):\n",
        "    def __init__(self, node_dim, edge_dim, out_dim):\n",
        "        super().__init__()\n",
        "        self.node_proj = nn.Linear(node_dim, out_dim)\n",
        "        self.edge_proj = nn.Linear(edge_dim, out_dim)\n",
        "        self.attn = nn.Linear(3 * out_dim, 1)\n",
        "        self.norm = LayerNorm(out_dim)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr):\n",
        "        src, dst = edge_index\n",
        "        h_src = self.node_proj(x[src])\n",
        "        h_dst = self.node_proj(x[dst])\n",
        "        e = self.edge_proj(edge_attr)\n",
        "\n",
        "        attn_input = torch.cat([h_src, h_dst, e], dim=-1)\n",
        "        alpha = torch.sigmoid(self.attn(attn_input))\n",
        "\n",
        "        messages = alpha * (h_src + e)\n",
        "        out = torch.zeros_like(self.node_proj(x))\n",
        "        out.index_add_(0, dst, messages)\n",
        "\n",
        "        return self.norm(out)"
      ],
      "metadata": {
        "id": "6NDcFC_phS6b"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Path Encoder"
      ],
      "metadata": {
        "id": "B_NygFtuhXJT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalPathEncoder(nn.Module):\n",
        "    def __init__(self, dim, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(dim, dim)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, h):\n",
        "        if self.training:\n",
        "            mask = torch.rand(h.size(0)) > self.dropout\n",
        "            h = h * mask.unsqueeze(-1)\n",
        "\n",
        "        return F.relu(self.fc(h)) + h   # residual"
      ],
      "metadata": {
        "id": "kMfl6520hVGf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Temporal Dropout prevents overfitting"
      ],
      "metadata": {
        "id": "lCRp9u8QhZ8X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TemporalDropout(nn.Module):\n",
        "    def __init__(self, p=0.2):\n",
        "        super().__init__()\n",
        "        self.p = p\n",
        "\n",
        "    def forward(self, x):\n",
        "        if not self.training:\n",
        "            return x\n",
        "        mask = torch.rand(x.size(1)) > self.p\n",
        "        return x[:, mask, :]\n"
      ],
      "metadata": {
        "id": "SGNDRbzVhYfH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Multi-Scale Temporal Encoder"
      ],
      "metadata": {
        "id": "ZIIq67S_heeL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiScaleTemporal(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.cnn = nn.Conv1d(dim, dim, kernel_size=2)\n",
        "        self.lstm = nn.LSTM(dim, dim, batch_first=True)\n",
        "        self.gru = nn.GRU(dim, dim, batch_first=True)\n",
        "\n",
        "        self.alpha = nn.Parameter(torch.ones(3))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, T, D]\n",
        "        cnn_out = self.cnn(x.transpose(1, 2)).mean(-1)\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        lstm_out = lstm_out[:, -1]\n",
        "\n",
        "        gru_out, _ = self.gru(x)\n",
        "        attn = torch.softmax(gru_out.mean(-1), dim=1)\n",
        "        gru_out = (gru_out * attn.unsqueeze(-1)).sum(1)\n",
        "\n",
        "        weights = torch.softmax(self.alpha, dim=0)\n",
        "        return (\n",
        "            weights[0] * cnn_out +\n",
        "            weights[1] * lstm_out +\n",
        "            weights[2] * gru_out\n",
        "        )\n"
      ],
      "metadata": {
        "id": "xcxR0rfxhcyj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention with confidence cap"
      ],
      "metadata": {
        "id": "n33vOi_Ehh8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MobilityAttention(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(1, dim)\n",
        "\n",
        "    def forward(self, h, speed):\n",
        "        gate = torch.sigmoid(self.fc(speed.unsqueeze(-1)))\n",
        "        gate = torch.clamp(gate, 0.3, 1.0)\n",
        "        return h * gate\n"
      ],
      "metadata": {
        "id": "UONkudGWhgQs"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NfPs3flThlYb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Bottleneck(nn.Module):\n",
        "    def __init__(self, dim, bottleneck_dim):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(dim, bottleneck_dim)\n",
        "        self.bn = BatchNorm1d(bottleneck_dim)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, h):\n",
        "        return self.dropout(self.bn(self.proj(h)))\n"
      ],
      "metadata": {
        "id": "kSo7di1Lhj20"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FailureHead(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(dim, 1)\n",
        "\n",
        "    def forward(self, h):\n",
        "        return torch.sigmoid(self.fc(h))\n",
        "\n",
        "\n",
        "class TypeHead(nn.Module):\n",
        "    def __init__(self, dim, num_classes=4):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(dim, num_classes)\n",
        "\n",
        "    def forward(self, h):\n",
        "        return self.fc(h)\n",
        "\n",
        "\n",
        "class TimeHead(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(dim, 1)\n",
        "\n",
        "    def forward(self, h):\n",
        "        return self.fc(h)\n",
        "\n",
        "\n",
        "class UncertaintyHead(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(dim, 1)\n",
        "\n",
        "    def forward(self, h):\n",
        "        return torch.clamp(F.softplus(self.fc(h)), min=1e-3)\n"
      ],
      "metadata": {
        "id": "wzlfxv4ZhoAd"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TGNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.egat = EGATLayer(15, 3, 64)\n",
        "        self.causal = CausalPathEncoder(64)\n",
        "        self.temporal = MultiScaleTemporal(64)\n",
        "        self.mobility = MobilityAttention(64)\n",
        "        self.bottleneck = Bottleneck(64, 32)\n",
        "\n",
        "        self.fail_head = FailureHead(32)\n",
        "        self.type_head = TypeHead(32)\n",
        "        self.time_head = TimeHead(32)\n",
        "        self.uncertainty_head = UncertaintyHead(32)\n",
        "\n",
        "    def forward(self, x_seq, edge_index, edge_attr, speed):\n",
        "        h_seq = []\n",
        "        for x in x_seq:\n",
        "            h = self.egat(x, edge_index, edge_attr)\n",
        "            h = self.causal(h)\n",
        "            h_seq.append(h)\n",
        "\n",
        "        h_seq = torch.stack(h_seq, dim=1)\n",
        "        h = self.temporal(h_seq)\n",
        "        h = self.mobility(h, speed)\n",
        "        h = self.bottleneck(h)\n",
        "\n",
        "        return {\n",
        "            \"failure\": self.fail_head(h),\n",
        "            \"type\": self.type_head(h),\n",
        "            \"time\": self.time_head(h),\n",
        "            \"uncertainty\": self.uncertainty_head(h)\n",
        "        }\n"
      ],
      "metadata": {
        "id": "A1MGOV9fhpWi"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def uncertainty_loss(y, mu, sigma):\n",
        "    return ((y - mu) ** 2) / (2 * sigma ** 2) + torch.log(sigma)\n"
      ],
      "metadata": {
        "id": "XytpOfpshqqg"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def total_loss(out, y_fail, y_type, y_time,\n",
        "               λ1=1.0, λ2=0.5, λ3=0.5, λ4=0.2, λ5=0.2):\n",
        "\n",
        "    L_fail = F.binary_cross_entropy(out[\"failure\"], y_fail)\n",
        "    L_type = F.cross_entropy(out[\"type\"], y_type)\n",
        "    L_time = F.smooth_l1_loss(out[\"time\"], y_time)\n",
        "    L_unc = uncertainty_loss(y_time, out[\"time\"], out[\"uncertainty\"]).mean()\n",
        "\n",
        "    return λ1*L_fail + λ2*L_type + λ3*L_time + λ5*L_unc\n"
      ],
      "metadata": {
        "id": "ZrhFZ8zQhry_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(model, batch, optimizer):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    out = model(**batch)\n",
        "    loss = total_loss(out,\n",
        "                      batch[\"y_fail\"],\n",
        "                      batch[\"y_type\"],\n",
        "                      batch[\"y_time\"])\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n"
      ],
      "metadata": {
        "id": "PskZv3Ofhs9N"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ibALVa8ehuey"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}